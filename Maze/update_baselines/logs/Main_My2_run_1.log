/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/gymnasium/envs/registration.py:642: UserWarning: [33mWARN: Overriding environment TrapMazeEnv already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yuxuanli2023 (yuxuanli2023-northwestern-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/xlx9645/failed/Maze/update_baselines/wandb/run-20241229_180908-36sgup5r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MyMethod_2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yuxuanli2023-northwestern-university/Main_1229
wandb: üöÄ View run at https://wandb.ai/yuxuanli2023-northwestern-university/Main_1229/runs/36sgup5r
/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/gymnasium/spaces/box.py:235: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/gymnasium/spaces/box.py:305: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
/home/xlx9645/failed/Maze/update_baselines/Main_My2.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  reward_net.load_state_dict(torch.load(reward_net_path, map_location=device))
Epoch 1/200, Loss: 1.6810275316238403
Epoch 2/200, Loss: 1.5746580362319946
Epoch 3/200, Loss: 1.4787663221359253
Epoch 4/200, Loss: 1.3914945125579834
Epoch 5/200, Loss: 1.3124924898147583
Epoch 6/200, Loss: 1.240671157836914
Epoch 7/200, Loss: 1.1747360229492188
Epoch 8/200, Loss: 1.1137025356292725
Epoch 9/200, Loss: 1.055606484413147
Epoch 10/200, Loss: 0.9992920160293579
Epoch 11/200, Loss: 0.9449246525764465
Epoch 12/200, Loss: 0.89311683177948
Epoch 13/200, Loss: 0.843796968460083
Epoch 14/200, Loss: 0.7971063852310181
Epoch 15/200, Loss: 0.7534003853797913
Epoch 16/200, Loss: 0.7129703164100647
Epoch 17/200, Loss: 0.6760612726211548
Epoch 18/200, Loss: 0.6427555680274963
Epoch 19/200, Loss: 0.613055408000946
Epoch 20/200, Loss: 0.5868864059448242
Epoch 21/200, Loss: 0.5640612840652466
Epoch 22/200, Loss: 0.5443096160888672
Epoch 23/200, Loss: 0.527194082736969
Epoch 24/200, Loss: 0.5122478604316711
Epoch 25/200, Loss: 0.4990702271461487
Epoch 26/200, Loss: 0.4871583878993988
Epoch 27/200, Loss: 0.47615548968315125
Epoch 28/200, Loss: 0.4657817482948303
Epoch 29/200, Loss: 0.4558292329311371
Epoch 30/200, Loss: 0.4462262690067291
Epoch 31/200, Loss: 0.43696457147598267
Epoch 32/200, Loss: 0.42807522416114807
Epoch 33/200, Loss: 0.41959235072135925
Epoch 34/200, Loss: 0.4115343689918518
Epoch 35/200, Loss: 0.4038568139076233
Epoch 36/200, Loss: 0.3965088427066803
Epoch 37/200, Loss: 0.38945192098617554
Epoch 38/200, Loss: 0.3826880156993866
Epoch 39/200, Loss: 0.3762068748474121
Epoch 40/200, Loss: 0.37004485726356506
Epoch 41/200, Loss: 0.36422157287597656
Epoch 42/200, Loss: 0.35878902673721313
Epoch 43/200, Loss: 0.3537980020046234
Epoch 44/200, Loss: 0.3492659032344818
Epoch 45/200, Loss: 0.34519776701927185
Epoch 46/200, Loss: 0.3415842056274414
Epoch 47/200, Loss: 0.33837074041366577
Epoch 48/200, Loss: 0.33552873134613037
Epoch 49/200, Loss: 0.3329800069332123
Epoch 50/200, Loss: 0.33065181970596313
Epoch 51/200, Loss: 0.32846781611442566
Epoch 52/200, Loss: 0.32634392380714417
Epoch 53/200, Loss: 0.3242212235927582
Epoch 54/200, Loss: 0.32205310463905334
Epoch 55/200, Loss: 0.3198309540748596
Epoch 56/200, Loss: 0.3175453841686249
Epoch 57/200, Loss: 0.31521469354629517
Epoch 58/200, Loss: 0.3128707706928253
Epoch 59/200, Loss: 0.31053683161735535
Epoch 60/200, Loss: 0.30823689699172974
Epoch 61/200, Loss: 0.3060012459754944
Epoch 62/200, Loss: 0.3038361966609955
Epoch 63/200, Loss: 0.3017407953739166
Epoch 64/200, Loss: 0.2997046709060669
Epoch 65/200, Loss: 0.2977570593357086
Epoch 66/200, Loss: 0.29590046405792236
Epoch 67/200, Loss: 0.2941245436668396
Epoch 68/200, Loss: 0.29241904616355896
Epoch 69/200, Loss: 0.29077500104904175
Epoch 70/200, Loss: 0.28922414779663086
Epoch 71/200, Loss: 0.28774669766426086
Epoch 72/200, Loss: 0.2863367795944214
Epoch 73/200, Loss: 0.2849888503551483
Epoch 74/200, Loss: 0.28372782468795776
Epoch 75/200, Loss: 0.2825562059879303
Epoch 76/200, Loss: 0.28145238757133484
Epoch 77/200, Loss: 0.28041768074035645
Epoch 78/200, Loss: 0.2794559597969055
Epoch 79/200, Loss: 0.2785508334636688
Epoch 80/200, Loss: 0.27769070863723755
Epoch 81/200, Loss: 0.2768564522266388
Epoch 82/200, Loss: 0.2760441303253174
Epoch 83/200, Loss: 0.27523836493492126
Epoch 84/200, Loss: 0.274444580078125
Epoch 85/200, Loss: 0.2736668586730957
Epoch 86/200, Loss: 0.27291029691696167
Epoch 87/200, Loss: 0.27218228578567505
Epoch 88/200, Loss: 0.2714865505695343
Epoch 89/200, Loss: 0.270835816860199
Epoch 90/200, Loss: 0.2702263295650482
Epoch 91/200, Loss: 0.2696404457092285
Epoch 92/200, Loss: 0.26906830072402954
Epoch 93/200, Loss: 0.26851633191108704
Epoch 94/200, Loss: 0.26797935366630554
Epoch 95/200, Loss: 0.26745712757110596
Epoch 96/200, Loss: 0.26695242524147034
Epoch 97/200, Loss: 0.26646214723587036
Epoch 98/200, Loss: 0.2659865617752075
Epoch 99/200, Loss: 0.26552054286003113
Epoch 100/200, Loss: 0.2650659382343292
Epoch 101/200, Loss: 0.26462119817733765
Epoch 102/200, Loss: 0.2641776502132416
Epoch 103/200, Loss: 0.26375341415405273
Epoch 104/200, Loss: 0.2632937431335449
Epoch 105/200, Loss: 0.2628626823425293
Epoch 106/200, Loss: 0.2624865770339966
Epoch 107/200, Loss: 0.2621157765388489
Epoch 108/200, Loss: 0.26176467537879944
Epoch 109/200, Loss: 0.2614220678806305
Epoch 110/200, Loss: 0.26108986139297485
Epoch 111/200, Loss: 0.2607690095901489
Epoch 112/200, Loss: 0.260467529296875
Epoch 113/200, Loss: 0.26018276810646057
Epoch 114/200, Loss: 0.2599070966243744
Epoch 115/200, Loss: 0.25964105129241943
Epoch 116/200, Loss: 0.2593839168548584
Epoch 117/200, Loss: 0.25914230942726135
Epoch 118/200, Loss: 0.25890854001045227
Epoch 119/200, Loss: 0.25868505239486694
Epoch 120/200, Loss: 0.2584649920463562
Epoch 121/200, Loss: 0.2582527995109558
Epoch 122/200, Loss: 0.25805211067199707
Epoch 123/200, Loss: 0.25786012411117554
Epoch 124/200, Loss: 0.25767073035240173
Epoch 125/200, Loss: 0.25748488306999207
Epoch 126/200, Loss: 0.25730428099632263
Epoch 127/200, Loss: 0.25712987780570984
Epoch 128/200, Loss: 0.2569562792778015
Epoch 129/200, Loss: 0.2567867934703827
Epoch 130/200, Loss: 0.25661543011665344
Epoch 131/200, Loss: 0.2564519941806793
Epoch 132/200, Loss: 0.2563013434410095
Epoch 133/200, Loss: 0.2561612129211426
Epoch 134/200, Loss: 0.2560208737850189
Epoch 135/200, Loss: 0.25587981939315796
Epoch 136/200, Loss: 0.2557298243045807
Epoch 137/200, Loss: 0.2555752396583557
Epoch 138/200, Loss: 0.2554265260696411
Epoch 139/200, Loss: 0.25528207421302795
Epoch 140/200, Loss: 0.2551353871822357
Epoch 141/200, Loss: 0.2549872100353241
Epoch 142/200, Loss: 0.25483760237693787
Epoch 143/200, Loss: 0.2546935975551605
Epoch 144/200, Loss: 0.25455790758132935
Epoch 145/200, Loss: 0.25442075729370117
Epoch 146/200, Loss: 0.2542850971221924
Epoch 147/200, Loss: 0.2541477680206299
Epoch 148/200, Loss: 0.2540084719657898
Epoch 149/200, Loss: 0.2538768947124481
Epoch 150/200, Loss: 0.25374922156333923
Epoch 151/200, Loss: 0.25362640619277954
Epoch 152/200, Loss: 0.25350144505500793
Epoch 153/200, Loss: 0.2533746361732483
Epoch 154/200, Loss: 0.2532487213611603
Epoch 155/200, Loss: 0.2531270980834961
Epoch 156/200, Loss: 0.2530065178871155
Epoch 157/200, Loss: 0.25287947058677673
Epoch 158/200, Loss: 0.2527456283569336
Epoch 159/200, Loss: 0.2526123523712158
Epoch 160/200, Loss: 0.2524864673614502
Epoch 161/200, Loss: 0.25234952569007874
Epoch 162/200, Loss: 0.25222495198249817
Epoch 163/200, Loss: 0.25209692120552063
Epoch 164/200, Loss: 0.2519685626029968
Epoch 165/200, Loss: 0.25183695554733276
Epoch 166/200, Loss: 0.25171008706092834
Epoch 167/200, Loss: 0.25158610939979553
Epoch 168/200, Loss: 0.2514518201351166
Epoch 169/200, Loss: 0.251322865486145
Epoch 170/200, Loss: 0.2511937618255615
Epoch 171/200, Loss: 0.25106754899024963
Epoch 172/200, Loss: 0.2509365379810333
Epoch 173/200, Loss: 0.25080883502960205
Epoch 174/200, Loss: 0.25067514181137085
Epoch 175/200, Loss: 0.25054609775543213
Epoch 176/200, Loss: 0.2504160702228546
Epoch 177/200, Loss: 0.25028735399246216
Epoch 178/200, Loss: 0.2501577138900757
Epoch 179/200, Loss: 0.25002744793891907
Epoch 180/200, Loss: 0.24989666044712067
Epoch 181/200, Loss: 0.24976706504821777
Epoch 182/200, Loss: 0.24963824450969696
Epoch 183/200, Loss: 0.2495131641626358
Epoch 184/200, Loss: 0.24938401579856873
Epoch 185/200, Loss: 0.24925543367862701
Epoch 186/200, Loss: 0.24912650883197784
Epoch 187/200, Loss: 0.24899345636367798
Epoch 188/200, Loss: 0.2488676756620407
Epoch 189/200, Loss: 0.24873411655426025
Epoch 190/200, Loss: 0.24860072135925293
Epoch 191/200, Loss: 0.24846921861171722
Epoch 192/200, Loss: 0.24833616614341736
Epoch 193/200, Loss: 0.24819721281528473
Epoch 194/200, Loss: 0.2480664700269699
Epoch 195/200, Loss: 0.24793384969234467
Epoch 196/200, Loss: 0.2477949857711792
Epoch 197/200, Loss: 0.24767112731933594
Epoch 198/200, Loss: 0.2475322186946869
Epoch 199/200, Loss: 0.24739611148834229
Epoch 200/200, Loss: 0.2472604662179947
Traceback (most recent call last):
  File "/home/xlx9645/failed/Maze/update_baselines/Main_My2.py", line 342, in <module>
    visualize_bcirl_reward_function(
  File "/home/xlx9645/failed/Maze/update_baselines/Main_My2.py", line 110, in visualize_bcirl_reward_function
    reward_net.load_state_dict(torch.load(reward_net_path, map_location=device))
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1360, in load
    return _load(
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1848, in _load
    result = unpickler.load()
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1812, in persistent_load
    typed_storage = load_tensor(
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1772, in load_tensor
    zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)
RuntimeError: PytorchStreamReader failed reading file data/2: file read failed
[1;34mwandb[0m: üöÄ View run [33mMyMethod_2[0m at: [34mhttps://wandb.ai/yuxuanli2023-northwestern-university/Main_1229/runs/36sgup5r[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241229_180908-36sgup5r/logs[0m
