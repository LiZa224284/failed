/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/gymnasium/envs/registration.py:642: UserWarning: [33mWARN: Overriding environment TrapMazeEnv already in registry.[0m
  logger.warn(f"Overriding environment {new_spec.id} already in registry.")
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yuxuanli2023 (yuxuanli2023-northwestern-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/xlx9645/failed/Maze/update_baselines/wandb/run-20241229_180908-non3egqt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MyMethod_2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yuxuanli2023-northwestern-university/Main_1229
wandb: üöÄ View run at https://wandb.ai/yuxuanli2023-northwestern-university/Main_1229/runs/non3egqt
/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/gymnasium/spaces/box.py:235: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/gymnasium/spaces/box.py:305: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
/home/xlx9645/failed/Maze/update_baselines/Main_My2.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  reward_net.load_state_dict(torch.load(reward_net_path, map_location=device))
Epoch 1/200, Loss: 1.4303797483444214
Epoch 2/200, Loss: 1.3214584589004517
Epoch 3/200, Loss: 1.2274577617645264
Epoch 4/200, Loss: 1.1430383920669556
Epoch 5/200, Loss: 1.066588282585144
Epoch 6/200, Loss: 0.9975023865699768
Epoch 7/200, Loss: 0.9341157078742981
Epoch 8/200, Loss: 0.8757144808769226
Epoch 9/200, Loss: 0.8220658302307129
Epoch 10/200, Loss: 0.7730942964553833
Epoch 11/200, Loss: 0.7284672260284424
Epoch 12/200, Loss: 0.68819260597229
Epoch 13/200, Loss: 0.6522350907325745
Epoch 14/200, Loss: 0.6205205321311951
Epoch 15/200, Loss: 0.592772901058197
Epoch 16/200, Loss: 0.5688302516937256
Epoch 17/200, Loss: 0.5481804013252258
Epoch 18/200, Loss: 0.5304579734802246
Epoch 19/200, Loss: 0.5152775645256042
Epoch 20/200, Loss: 0.5022553205490112
Epoch 21/200, Loss: 0.49056723713874817
Epoch 22/200, Loss: 0.4795970916748047
Epoch 23/200, Loss: 0.4689747989177704
Epoch 24/200, Loss: 0.4584081470966339
Epoch 25/200, Loss: 0.44775882363319397
Epoch 26/200, Loss: 0.4371030330657959
Epoch 27/200, Loss: 0.4265763461589813
Epoch 28/200, Loss: 0.41639769077301025
Epoch 29/200, Loss: 0.4068152904510498
Epoch 30/200, Loss: 0.3980492353439331
Epoch 31/200, Loss: 0.390177458524704
Epoch 32/200, Loss: 0.38314685225486755
Epoch 33/200, Loss: 0.3768976032733917
Epoch 34/200, Loss: 0.3713369071483612
Epoch 35/200, Loss: 0.3663230836391449
Epoch 36/200, Loss: 0.36173391342163086
Epoch 37/200, Loss: 0.35746535658836365
Epoch 38/200, Loss: 0.3534432649612427
Epoch 39/200, Loss: 0.3496474027633667
Epoch 40/200, Loss: 0.346105694770813
Epoch 41/200, Loss: 0.342854768037796
Epoch 42/200, Loss: 0.3398972153663635
Epoch 43/200, Loss: 0.33721795678138733
Epoch 44/200, Loss: 0.33473438024520874
Epoch 45/200, Loss: 0.3323500156402588
Epoch 46/200, Loss: 0.3299623131752014
Epoch 47/200, Loss: 0.32746419310569763
Epoch 48/200, Loss: 0.32483139634132385
Epoch 49/200, Loss: 0.32206660509109497
Epoch 50/200, Loss: 0.3192327320575714
Epoch 51/200, Loss: 0.3164212703704834
Epoch 52/200, Loss: 0.31371089816093445
Epoch 53/200, Loss: 0.31115418672561646
Epoch 54/200, Loss: 0.3087986707687378
Epoch 55/200, Loss: 0.30668920278549194
Epoch 56/200, Loss: 0.3047977089881897
Epoch 57/200, Loss: 0.3030649423599243
Epoch 58/200, Loss: 0.30146244168281555
Epoch 59/200, Loss: 0.29995593428611755
Epoch 60/200, Loss: 0.29849302768707275
Epoch 61/200, Loss: 0.29705026745796204
Epoch 62/200, Loss: 0.2956528663635254
Epoch 63/200, Loss: 0.29429227113723755
Epoch 64/200, Loss: 0.2929855287075043
Epoch 65/200, Loss: 0.29173603653907776
Epoch 66/200, Loss: 0.2905581593513489
Epoch 67/200, Loss: 0.2894415259361267
Epoch 68/200, Loss: 0.28838086128234863
Epoch 69/200, Loss: 0.2873619496822357
Epoch 70/200, Loss: 0.28639280796051025
Epoch 71/200, Loss: 0.2854585647583008
Epoch 72/200, Loss: 0.28453314304351807
Epoch 73/200, Loss: 0.2836044430732727
Epoch 74/200, Loss: 0.2826826274394989
Epoch 75/200, Loss: 0.28176045417785645
Epoch 76/200, Loss: 0.28081607818603516
Epoch 77/200, Loss: 0.2798703908920288
Epoch 78/200, Loss: 0.2789264917373657
Epoch 79/200, Loss: 0.27800875902175903
Epoch 80/200, Loss: 0.2771284878253937
Epoch 81/200, Loss: 0.2762737274169922
Epoch 82/200, Loss: 0.27545100450515747
Epoch 83/200, Loss: 0.2746562957763672
Epoch 84/200, Loss: 0.27387842535972595
Epoch 85/200, Loss: 0.27312082052230835
Epoch 86/200, Loss: 0.27238523960113525
Epoch 87/200, Loss: 0.27166926860809326
Epoch 88/200, Loss: 0.27098098397254944
Epoch 89/200, Loss: 0.2703227698802948
Epoch 90/200, Loss: 0.26968592405319214
Epoch 91/200, Loss: 0.26906660199165344
Epoch 92/200, Loss: 0.26847803592681885
Epoch 93/200, Loss: 0.26791679859161377
Epoch 94/200, Loss: 0.26736894249916077
Epoch 95/200, Loss: 0.26683908700942993
Epoch 96/200, Loss: 0.2663320302963257
Epoch 97/200, Loss: 0.26584315299987793
Epoch 98/200, Loss: 0.2653730809688568
Epoch 99/200, Loss: 0.2649174928665161
Epoch 100/200, Loss: 0.2644853889942169
Epoch 101/200, Loss: 0.26407426595687866
Epoch 102/200, Loss: 0.2636883854866028
Epoch 103/200, Loss: 0.2633153200149536
Epoch 104/200, Loss: 0.2629586160182953
Epoch 105/200, Loss: 0.2626224756240845
Epoch 106/200, Loss: 0.26229748129844666
Epoch 107/200, Loss: 0.2619817852973938
Epoch 108/200, Loss: 0.2616770267486572
Epoch 109/200, Loss: 0.26138797402381897
Epoch 110/200, Loss: 0.2611173987388611
Epoch 111/200, Loss: 0.2608513832092285
Epoch 112/200, Loss: 0.26058709621429443
Epoch 113/200, Loss: 0.2603326439857483
Epoch 114/200, Loss: 0.2600845992565155
Epoch 115/200, Loss: 0.25984638929367065
Epoch 116/200, Loss: 0.25962793827056885
Epoch 117/200, Loss: 0.2594310939311981
Epoch 118/200, Loss: 0.2592368721961975
Epoch 119/200, Loss: 0.25904715061187744
Epoch 120/200, Loss: 0.25885868072509766
Epoch 121/200, Loss: 0.2586727440357208
Epoch 122/200, Loss: 0.2584904730319977
Epoch 123/200, Loss: 0.25831228494644165
Epoch 124/200, Loss: 0.2581492066383362
Epoch 125/200, Loss: 0.25800007581710815
Epoch 126/200, Loss: 0.2578541040420532
Epoch 127/200, Loss: 0.25770828127861023
Epoch 128/200, Loss: 0.25756049156188965
Epoch 129/200, Loss: 0.25741294026374817
Epoch 130/200, Loss: 0.2572636306285858
Epoch 131/200, Loss: 0.25711482763290405
Epoch 132/200, Loss: 0.2569692134857178
Epoch 133/200, Loss: 0.2568230926990509
Epoch 134/200, Loss: 0.25667884945869446
Epoch 135/200, Loss: 0.256537526845932
Epoch 136/200, Loss: 0.2564007043838501
Epoch 137/200, Loss: 0.25626420974731445
Epoch 138/200, Loss: 0.25613105297088623
Epoch 139/200, Loss: 0.2559967041015625
Epoch 140/200, Loss: 0.25586047768592834
Epoch 141/200, Loss: 0.25572216510772705
Epoch 142/200, Loss: 0.2555829584598541
Epoch 143/200, Loss: 0.2554439604282379
Epoch 144/200, Loss: 0.2553083300590515
Epoch 145/200, Loss: 0.2551811635494232
Epoch 146/200, Loss: 0.255054771900177
Epoch 147/200, Loss: 0.25492528080940247
Epoch 148/200, Loss: 0.2547968626022339
Epoch 149/200, Loss: 0.25467154383659363
Epoch 150/200, Loss: 0.25453928112983704
Epoch 151/200, Loss: 0.2544059753417969
Epoch 152/200, Loss: 0.2542743980884552
Epoch 153/200, Loss: 0.2541431188583374
Epoch 154/200, Loss: 0.25401169061660767
Epoch 155/200, Loss: 0.253884494304657
Epoch 156/200, Loss: 0.2537522315979004
Epoch 157/200, Loss: 0.2536167800426483
Epoch 158/200, Loss: 0.2534785866737366
Epoch 159/200, Loss: 0.253347784280777
Epoch 160/200, Loss: 0.25321337580680847
Epoch 161/200, Loss: 0.2530803084373474
Epoch 162/200, Loss: 0.25295549631118774
Epoch 163/200, Loss: 0.2528429925441742
Epoch 164/200, Loss: 0.25273340940475464
Epoch 165/200, Loss: 0.252618670463562
Epoch 166/200, Loss: 0.25250107049942017
Epoch 167/200, Loss: 0.2523835003376007
Epoch 168/200, Loss: 0.25226637721061707
Epoch 169/200, Loss: 0.25215014815330505
Epoch 170/200, Loss: 0.2520318925380707
Epoch 171/200, Loss: 0.25191229581832886
Epoch 172/200, Loss: 0.2517898976802826
Epoch 173/200, Loss: 0.25167056918144226
Epoch 174/200, Loss: 0.25154998898506165
Epoch 175/200, Loss: 0.251425176858902
Epoch 176/200, Loss: 0.25128626823425293
Epoch 177/200, Loss: 0.2511352002620697
Epoch 178/200, Loss: 0.2509761452674866
Epoch 179/200, Loss: 0.2508346140384674
Epoch 180/200, Loss: 0.2507127821445465
Epoch 181/200, Loss: 0.25059401988983154
Epoch 182/200, Loss: 0.25047504901885986
Epoch 183/200, Loss: 0.2503517270088196
Epoch 184/200, Loss: 0.2502293586730957
Epoch 185/200, Loss: 0.2501102685928345
Epoch 186/200, Loss: 0.24999204277992249
Epoch 187/200, Loss: 0.24986927211284637
Epoch 188/200, Loss: 0.24974948167800903
Epoch 189/200, Loss: 0.2496316283941269
Epoch 190/200, Loss: 0.24951347708702087
Epoch 191/200, Loss: 0.24939517676830292
Epoch 192/200, Loss: 0.24927808344364166
Epoch 193/200, Loss: 0.2491612285375595
Epoch 194/200, Loss: 0.24904212355613708
Epoch 195/200, Loss: 0.24892623722553253
Epoch 196/200, Loss: 0.24880988895893097
Epoch 197/200, Loss: 0.24868996441364288
Epoch 198/200, Loss: 0.24856993556022644
Epoch 199/200, Loss: 0.24845075607299805
Epoch 200/200, Loss: 0.24833086133003235
Traceback (most recent call last):
  File "/home/xlx9645/failed/Maze/update_baselines/Main_My2.py", line 342, in <module>
    visualize_bcirl_reward_function(
  File "/home/xlx9645/failed/Maze/update_baselines/Main_My2.py", line 110, in visualize_bcirl_reward_function
    reward_net.load_state_dict(torch.load(reward_net_path, map_location=device))
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1360, in load
    return _load(
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1848, in _load
    result = unpickler.load()
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1812, in persistent_load
    typed_storage = load_tensor(
  File "/home/xlx9645/miniconda3/envs/maze/lib/python3.9/site-packages/torch/serialization.py", line 1772, in load_tensor
    zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)
RuntimeError: PytorchStreamReader failed reading file data/2: file read failed
[1;34mwandb[0m: üöÄ View run [33mMyMethod_2[0m at: [34mhttps://wandb.ai/yuxuanli2023-northwestern-university/Main_1229/runs/non3egqt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241229_180908-non3egqt/logs[0m
